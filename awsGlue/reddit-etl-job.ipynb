{
	"metadata": {
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		},
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "import os\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue import DynamicFrame\nfrom awsglue.transforms import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import functions as F\nimport sys",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args[\"JOB_NAME\"], args)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "bucketName = 'reddit-koroomvn-bucket'\ndbName = 'redditdb'",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "dyf =  glueContext.create_dynamic_frame_from_options(\n    connection_type='s3'\n    , connection_options={\n        'paths': [f's3://{bucketName}/raw/']\n        , 'recurse': True\n    }\n    , format='csv'\n    , format_options={\n        'withHeader': True\n    }\n)\n\ndyfApplyMapping = dyf.apply_mapping([\n    ('id', 'string', 'id', 'string')\n    , ('title', 'string', 'title', 'string')\n    , ('score', 'string', 'score', 'integer')\n    , ('num_comments', 'string', 'num_comments', 'integer')\n    , ('author', 'string', 'author', 'string')\n    , ('created_utc', 'string', 'created_utc', 'timestamp')\n    , ('url', 'string', 'url', 'string')\n    , ('over_18', 'string', 'over_18', 'boolean')\n    , ('edited', 'string', 'edited', 'boolean')\n    , ('spoiler', 'string', 'spoiler', 'boolean')\n    , ('stickied', 'string', 'stickied', 'boolean')\n])\n\ndef addColumns(rec):\n    rec['ess_updated'] = (\n        str(rec['edited']) \n        + '-' + str(rec['spoiler']) \n        + '-' + str(rec['stickied'])\n        )\n    \n    rec['year'] = rec['created_utc'].year\n    rec['month'] = rec['created_utc'].month\n    rec['day'] = rec['created_utc'].day\n    \n    return rec\n\ndyfMap = dyfApplyMapping.map(f=addColumns)\n\ndyfDropFields = dyfMap.drop_fields(['edited', 'spoiler', 'stickied'])\ndyfDropFields.show()\nsink = glueContext.getSink(\n    connection_type='s3'\n    , path=f's3://{bucketName}/transformed/'\n    , enableUpdateCatalog=True\n    , updateBehavior='UPDATE_IN_DATABASE'\n    , partitionKeys=['year', 'month', 'day']\n)\n\nsink.setFormat(\n    format='csv'\n    , separator=','\n    , writeHeader=False\n)\n\nsink.setCatalogInfo(\n    catalogDatabase=dbName\n    , catalogTableName='transformed'\n)\nsink.writeFrame(\n    dynamic_frame=dyfDropFields\n)",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "job.commit()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}